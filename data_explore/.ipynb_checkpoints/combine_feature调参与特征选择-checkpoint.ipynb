{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义全局函数便于直接调用处理\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import boxcox\n",
    "############目录定义#################################\n",
    "datapath = 'D:/outsourcing/data/'\n",
    "featurepath = 'D:/outsourcing/feature/'\n",
    "resultpath = 'D:/outsourcing/result/'\n",
    "tmppath = 'D:/outsourcing/tmp/'\n",
    "\n",
    "\n",
    "###############函数定义################################\n",
    "# reduce memory\n",
    "def read_csv(file_name, num_rows=None):\n",
    "    if num_rows is None:\n",
    "        return pd.read_csv(file_name)\n",
    "    return pd.read_csv(file_name, nrows=num_rows)\n",
    "\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "\n",
    "def evaluation(x_train, x_test, y_train, y_test, str):\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    # knn = KNeighborsClassifier(n_neighbors=5, p=1)\n",
    "    bayes = GaussianNB()\n",
    "    tree = DecisionTreeClassifier()\n",
    "    svm = SVC()\n",
    "    LR = LogisticRegression()\n",
    "    model_list = [bayes, tree, svm, LR]\n",
    "    model_name = ['bayes', 'tree', 'svm', 'LR']\n",
    "    f = open('../result/' + str + '.txt', mode='x')\n",
    "    for i in range(len(model_list)):\n",
    "        np.random.seed(0)\n",
    "        model = model_list[i]\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "        y_pred2 = model.predict(x_train)\n",
    "        print(\"###################\" + model_name[i] + \"#########################\", file=f)\n",
    "        from sklearn.metrics import balanced_accuracy_score, f1_score, precision_score, recall_score, \\\n",
    "            accuracy_score, roc_auc_score, confusion_matrix\n",
    "        print(\"balanced_accuracy_score=\", balanced_accuracy_score(y_pred=y_pred, y_true=y_test),\n",
    "              balanced_accuracy_score(y_pred=y_pred2, y_true=y_train), file=f)\n",
    "        print(\"f1=\", f1_score(y_pred=y_pred, y_true=y_test), f1_score(y_pred=y_pred2, y_true=y_train), file=f)\n",
    "        print(\"precision_score=\", precision_score(y_pred=y_pred, y_true=y_test),\n",
    "              precision_score(y_pred=y_pred2, y_true=y_train), file=f)\n",
    "        print(\"recall_score=\", recall_score(y_pred=y_pred, y_true=y_test), recall_score(y_pred=y_pred2, y_true=y_train),\n",
    "              file=f)\n",
    "        print(\"accuracy=\", accuracy_score(y_pred=y_pred, y_true=y_test), accuracy_score(y_pred=y_pred2, y_true=y_train),\n",
    "              file=f)\n",
    "        print(\"auc=\", roc_auc_score(y_true=y_test, y_score=y_pred), roc_auc_score(y_true=y_train, y_score=y_pred2),\n",
    "              file=f)\n",
    "        print(\"#####混淆矩阵#########\", file=f)\n",
    "        print(confusion_matrix(y_true=y_test, y_pred=y_pred), confusion_matrix(y_true=y_train, y_pred=y_pred2), file=f)\n",
    "    return\n",
    "def getTrainTest(X, Y):\n",
    "    global x_train, x_test, y_train, y_test\n",
    "    # 会员编号等，等下仔细去查看所有取值数量超过100的特征\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    kfold = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "    for train_index, test_index in kfold.split(X, Y):\n",
    "        x_train = X.loc[train_index]\n",
    "        x_test = X.loc[test_index]\n",
    "        y_train = Y.loc[train_index]\n",
    "        y_test = Y.loc[test_index]\n",
    "        break\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def getTrainTest_np(X, Y):\n",
    "    global x_train, x_test, y_train, y_test\n",
    "    # 会员编号等，等下仔细去查看所有取值数量超过100的特征\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    kfold = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "    for train_index, test_index in kfold.split(X, Y):\n",
    "        x_train = X[train_index]\n",
    "        x_test = X[test_index]\n",
    "        y_train = Y[train_index]\n",
    "        y_test = Y[test_index]\n",
    "        break\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def minmax_target(X_train, X_test, Y_train, continue_list, discrete_list):\n",
    "    import category_encoders as ce\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    encoder = ce.LeaveOneOutEncoder(cols=discrete_list, drop_invariant=False).fit(X_train, Y_train)\n",
    "    minmax = MinMaxScaler()\n",
    "    train = pd.concat([X_train, X_test])\n",
    "    minmax.fit(train[continue_list])\n",
    "\n",
    "    X_train = encoder.transform(X_train)  # 基于训练集得到编码器\n",
    "    X_test = encoder.transform(X_test)\n",
    "    X_train[continue_list] = minmax.transform(X_train[continue_list])\n",
    "    X_test[continue_list] = minmax.transform(X_test[continue_list])\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "def target(X_train, X_test, Y_train, discrete_list):\n",
    "    import category_encoders as ce\n",
    "\n",
    "    encoder = ce.LeaveOneOutEncoder(cols=discrete_list, drop_invariant=False).fit(X_train, Y_train)\n",
    "\n",
    "    X_train = encoder.transform(X_train)  # 基于训练集得到编码器\n",
    "    X_test = encoder.transform(X_test)\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "def minmax(X_train, X_test, continue_list):\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    minmax = MinMaxScaler()\n",
    "    train = pd.concat([X_train, X_test])\n",
    "    minmax.fit(train[continue_list])\n",
    "    X_train[continue_list] = minmax.transform(X_train[continue_list])\n",
    "    X_test[continue_list] = minmax.transform(X_test[continue_list])\n",
    "    return X_train, X_test\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def feature_selection(train, train_sel, target):\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0, n_jobs=-1)\n",
    "    \n",
    "    scores = cross_val_score(clf, train, target, cv=5,scoring='f1')\n",
    "    scores_sel = cross_val_score(clf, train_sel, target, cv=5,scoring='f1')\n",
    "    \n",
    "    print(\"No Select Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))     \n",
    "    print(\"Features Select Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combine_feature数据集尝试,基于树模型的特征选择！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = reduce_mem_usage(read_csv(tmppath + 'sub/sub_train_all.csv',100))\n",
    "test = reduce_mem_usage(read_csv(tmppath + 'sub/sub_test_all.csv',10))\n",
    "\n",
    "X_train = train.drop(['emd_lable2'], axis=1)  # 去除部分取值过多的离散型特征\n",
    "Y_train = train['emd_lable2'].astype(int)\n",
    "\n",
    "discrete_list = ['seg_flight', 'seg_cabin', 'pref_orig_m6_2', 'pref_line_y1_2',\n",
    "                 'pref_line_y1_3', 'pref_line_y2_2', 'pref_line_y2_3', 'pref_line_y3_3'\n",
    "    , 'pref_line_y3_4', 'pref_line_y3_5', 'pref_aircraft_y3_3', 'pref_city_y1_2',\n",
    "                 'pref_city_y3_4', 'pref_dest_city_m6', 'pref_dest_city_y3'\n",
    "    , 'pref_month_y3_1', 'seg_dep_time_month']  # 训练中需要剔除的特征都是离散型的特征\n",
    "feature_list = X_train.columns.tolist()\n",
    "continue_list = list(set(feature_list) - set(discrete_list))\n",
    "\n",
    "X_train, test = minmax_target(X_train, test, Y_train, continue_list, discrete_list)  # 离散值编码与连续特征归一化\n",
    "\n",
    "del test, train\n",
    "x_train, x_test, y_train, y_test = getTrainTest(X_train, Y_train)  # 线下验证，80%训练集，20%验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "clf = lightgbm\n",
    "\n",
    "train_matrix = clf.Dataset(x_train, label=y_train)\n",
    "test_matrix = clf.Dataset(x_test, label=y_test)\n",
    "params = {\n",
    "          'boosting_type': 'gbdt',\n",
    "          #'boosting_type': 'dart',\n",
    "          'objective': 'multiclass',\n",
    "          'metric': 'multi_logloss',\n",
    "          'min_child_weight': 1.5,\n",
    "          'num_leaves': 2**5,\n",
    "          'lambda_l2': 10,\n",
    "          'subsample': 0.7,\n",
    "          'colsample_bytree': 0.7,\n",
    "          'colsample_bylevel': 0.7,\n",
    "          'learning_rate': 0.03,\n",
    "          'tree_method': 'exact',\n",
    "          'seed': 2017,\n",
    "          \"num_class\": 2,\n",
    "          'silent': True,\n",
    "          }\n",
    "num_round = 10000\n",
    "early_stopping_rounds = 100\n",
    "model = clf.train(params, \n",
    "                  train_matrix,\n",
    "                  num_round,\n",
    "                  valid_sets=test_matrix,\n",
    "                  early_stopping_rounds=early_stopping_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(y=X_train.columns, x=clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_transform(train, test, model, topK):\n",
    "    train_df = pd.DataFrame(train)\n",
    "    train_df.columns = range(train.shape[1])\n",
    "    \n",
    "    test_df = pd.DataFrame(test)\n",
    "    test_df.columns = range(test.shape[1])\n",
    "    \n",
    "    features_import = pd.DataFrame()\n",
    "    features_import['importance'] = model.feature_importance()\n",
    "    features_import['col'] = range(train.shape[1])\n",
    "    \n",
    "    features_import = features_import.sort_values(['importance'],ascending=0).head(topK)\n",
    "    sel_col = list(features_import.col)\n",
    "    \n",
    "    train_sel = train_df[sel_col]\n",
    "    test_sel = test_df[sel_col]\n",
    "    return train_sel, test_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sel, test_sel = lgb_transform(X_train, test, model, 20)\n",
    "print('训练数据未特征筛选维度', train.shape)\n",
    "print('训练数据特征筛选维度后', train_sel.shape)\n",
    "feature_selection(X_train, train_sel, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
